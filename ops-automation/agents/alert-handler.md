# Alert Handler Agent

## ì—­í• 
ì•ŒëŒì„ ìˆ˜ì‹ í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ íŒë‹¨í•˜ì—¬ ì ì ˆí•œ ì¡°ì¹˜ë¥¼ ì·¨í•©ë‹ˆë‹¤.

## ì‹¤í–‰ ëª¨ë“œ
- **ì„¸ì…˜ íƒ€ì…**: Isolated (ì´ë²¤íŠ¸ ê¸°ë°˜)
- **íŠ¸ë¦¬ê±°**: Metrics Collector ë˜ëŠ” ì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œ
- **ëª¨ë¸**: claude-sonnet-4-5

## ì…ë ¥

### ë‚´ë¶€ ì•ŒëŒ (Metrics Collector)
```json
{
  "source": "metrics-collector",
  "timestamp": "2026-02-02T11:22:00+09:00",
  "alerts": [
    {
      "severity": "critical",
      "metric": "cpu_percent",
      "value": 95.2,
      "threshold": 90,
      "message": "CPU usage critical"
    }
  ]
}
```

### ì™¸ë¶€ ì•ŒëŒ (Prometheus Alertmanager)
```json
{
  "source": "prometheus",
  "alerts": [
    {
      "status": "firing",
      "labels": {
        "alertname": "HighCPU",
        "instance": "macbook-pro",
        "severity": "warning"
      },
      "annotations": {
        "summary": "CPU usage above 70%"
      }
    }
  ]
}
```

## ì•ŒëŒ ì²˜ë¦¬ í”Œë¡œìš°

```javascript
async function handleAlerts(input) {
  const alerts = normalizeAlerts(input);
  
  for (const alert of alerts) {
    // 1. ì¤‘ë³µ í•„í„°ë§
    if (isDuplicate(alert)) {
      console.log(`Skipping duplicate alert: ${alert.metric}`);
      continue;
    }
    
    // 2. ì‹¬ê°ë„ ë¶„ë¥˜
    const severity = classifySeverity(alert);
    
    // 3. ì¡°ì¹˜ ê²°ì •
    switch (severity) {
      case 'critical':
        await handleCriticalAlert(alert);
        break;
      case 'warning':
        await handleWarningAlert(alert);
        break;
      case 'info':
        await logAlert(alert);
        break;
    }
  }
}
```

## ì‹¬ê°ë„ ë¶„ë¥˜

```javascript
function classifySeverity(alert) {
  // 1. ëª…ì‹œì  ì‹¬ê°ë„ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
  if (alert.severity) {
    return alert.severity;
  }
  
  // 2. ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¶„ë¥˜
  const thresholds = JSON.parse(readFile('config/alert-thresholds.json'));
  const threshold = thresholds[alert.metric];
  
  if (!threshold) {
    return 'info';
  }
  
  if (alert.value >= threshold.critical) {
    return 'critical';
  } else if (alert.value >= threshold.warning) {
    return 'warning';
  } else {
    return 'info';
  }
}
```

## Critical ì•ŒëŒ ì²˜ë¦¬

```javascript
async function handleCriticalAlert(alert) {
  // 1. ì¸ì‹œë˜íŠ¸ ìƒì„±
  const incident = createIncident(alert);
  
  // 2. AutoHeal ì‹œë„
  if (canAutoHeal(alert)) {
    const result = await sessions_spawn({
      agentId: 'autoheal',
      task: `Heal incident: ${JSON.stringify(incident)}`
    });
    
    if (result.status === 'resolved') {
      // ë³µêµ¬ ì„±ê³µ - ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ë§Œ
      await notifyUser(`âœ… AutoHeal ì„±ê³µ: ${alert.message}`);
      return;
    }
  }
  
  // 3. ë³µêµ¬ ì‹¤íŒ¨ ë˜ëŠ” AutoHeal ë¶ˆê°€ - ì—ìŠ¤ì»¬ë ˆì´ì…˜
  await escalate(incident);
}
```

## Warning ì•ŒëŒ ì²˜ë¦¬

```javascript
async function handleWarningAlert(alert) {
  // 1. ë¡œê·¸ ê¸°ë¡
  await logAlert(alert);
  
  // 2. ì§€ì† ì‹œê°„ í™•ì¸
  const duration = getAlertDuration(alert);
  
  if (duration > 300) { // 5ë¶„ ì´ìƒ ì§€ì†
    // Criticalë¡œ ìŠ¹ê²©
    alert.severity = 'critical';
    await handleCriticalAlert(alert);
  } else {
    // ëª¨ë‹ˆí„°ë§ë§Œ ê³„ì†
    console.log(`Warning alert active for ${duration}s: ${alert.message}`);
  }
}
```

## ì¤‘ë³µ í•„í„°ë§

```javascript
// alerts/dedup.jsonì— ìµœê·¼ ì•ŒëŒ ê¸°ë¡
function isDuplicate(alert) {
  const dedupState = JSON.parse(readFile('alerts/dedup.json') || '{}');
  const key = `${alert.metric}_${alert.severity}`;
  
  const lastSeen = dedupState[key];
  const now = Date.now();
  
  // 5ë¶„ ì´ë‚´ì— ê°™ì€ ì•ŒëŒì´ ìˆì—ˆìœ¼ë©´ ì¤‘ë³µ
  if (lastSeen && (now - lastSeen) < 300000) {
    return true;
  }
  
  // ìƒíƒœ ì—…ë°ì´íŠ¸
  dedupState[key] = now;
  writeFile('alerts/dedup.json', JSON.stringify(dedupState));
  
  return false;
}
```

## ì—ìŠ¤ì»¬ë ˆì´ì…˜

```javascript
async function escalate(incident) {
  const config = JSON.parse(readFile('config/escalation-policy.json'));
  
  // 1. ì‹œê°„ëŒ€ í™•ì¸
  const now = new Date();
  const hour = now.getHours();
  
  if (hour >= 23 || hour < 8) {
    // ì•¼ê°„ - ê¸´ê¸‰í•œ ê²ƒë§Œ
    if (incident.severity !== 'critical') {
      console.log('Non-critical incident during quiet hours - delayed');
      return;
    }
  }
  
  // 2. ë‹´ë‹¹ì ê²°ì •
  const oncall = getCurrentOncall(config);
  
  // 3. ì•Œë¦¼ ì „ì†¡
  await notifyUser(`
ğŸš¨ ì¸ì‹œë˜íŠ¸ ì—ìŠ¤ì»¬ë ˆì´ì…˜

**ì‹¬ê°ë„**: ${incident.severity}
**ë©”íŠ¸ë¦­**: ${incident.metric}
**í˜„ì¬ê°’**: ${incident.value}
**ì„ê³„ê°’**: ${incident.threshold}

**ì¡°ì¹˜**: ${incident.auto_heal_attempted ? 'AutoHeal ì‹œë„í–ˆìœ¼ë‚˜ ì‹¤íŒ¨' : 'ìˆ˜ë™ ê°œì… í•„ìš”'}

ìƒì„¸: incidents/${incident.id}.md
  `);
  
  // 4. PagerDuty/Opsgenie í†µí•©
  if (config.pagerduty_enabled) {
    await triggerPagerDuty(incident, oncall);
  }
}
```

## ì•ŒëŒ ê·¸ë£¹í™”

ì—¬ëŸ¬ ê´€ë ¨ ì•ŒëŒì„ ê·¸ë£¹í™”:
```javascript
function groupAlerts(alerts) {
  const groups = {
    'system_overload': [],
    'service_down': [],
    'network_issue': [],
    'other': []
  };
  
  for (const alert of alerts) {
    if (alert.metric.includes('cpu') || alert.metric.includes('memory')) {
      groups.system_overload.push(alert);
    } else if (alert.metric.includes('process')) {
      groups.service_down.push(alert);
    } else if (alert.metric.includes('network')) {
      groups.network_issue.push(alert);
    } else {
      groups.other.push(alert);
    }
  }
  
  return groups;
}
```

## ì„¤ì •

### escalation-policy.json
```json
{
  "pagerduty_enabled": false,
  "quiet_hours": {
    "start": "23:00",
    "end": "08:00"
  },
  "oncall_schedule": [
    {
      "day": "weekday",
      "contact": "imessage:+821062515961"
    },
    {
      "day": "weekend",
      "contact": "email:oncall@example.com"
    }
  ],
  "dedup_window_seconds": 300,
  "auto_heal_enabled": true
}
```

## ì™¸ë¶€ í†µí•©

### Webhook ìˆ˜ì‹ 
```bash
# Express ì„œë²„ë¡œ ì™¸ë¶€ ì•ŒëŒ ìˆ˜ì‹ 
curl -X POST http://localhost:18789/alerts/webhook \
  -H 'Content-Type: application/json' \
  -d '{
    "source": "datadog",
    "alert": {
      "title": "High Memory Usage",
      "severity": "warning",
      "value": 85
    }
  }'
```

### Alertmanager í†µí•©
```yaml
# alertmanager.yml
receivers:
  - name: 'openclaw'
    webhook_configs:
      - url: 'http://localhost:18789/alerts/webhook'
        send_resolved: true
```

## í†µê³„ ì¶”ì 

```javascript
// alerts/stats.json
{
  "total_alerts": 124,
  "by_severity": {
    "critical": 12,
    "warning": 56,
    "info": 56
  },
  "by_metric": {
    "cpu_percent": 45,
    "disk_usage": 32,
    "api_latency": 28,
    "process_down": 19
  },
  "auto_heal_success_rate": 0.917,
  "average_resolution_time_seconds": 150
}
```

## í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ì•ŒëŒ ì „ì†¡
openclaw agents spawn alert-handler --task '{
  "source": "test",
  "alerts": [{
    "severity": "warning",
    "metric": "cpu_percent",
    "value": 75,
    "threshold": 70,
    "message": "Test alert"
  }]
}'
```
